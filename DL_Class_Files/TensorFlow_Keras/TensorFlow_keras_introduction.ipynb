{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TensorFlow_keras_introduction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNuusJv9rWij2SlevjtRxRE"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"euVfXcPF0f6C"},"source":["[statinfer.com](https://statinfer.com/)\n"]},{"cell_type":"markdown","metadata":{"id":"wWEKV2Um1ZrD"},"source":["# Installing TensorFlow and Keras"]},{"cell_type":"code","metadata":{"id":"VlW5FE3Z0VAE"},"source":["### !pip install -q --upgrade tensorflow\n","\n","#Installing a specific version of tensorflow\n","#!pip install tensorflow==2.0.0\n","\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8V_JojM1hd1"},"source":["import numpy as np\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1InJsgo-1rgW"},"source":["#This step is for data creation, x and y\n","import numpy as np\n","x_train= np.array(range(5000,5100)).reshape(-1,1)\n","\n","\n","y_train=[3*i+np.random.normal(500, 10) for i in x_train]\n","\n","import matplotlib.pyplot as plt\n","plt.title(\"x_train vs y_train data\")\n","plt.plot(x_train, y_train, 'b.')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdC96j3Z13wW"},"source":["# Linear Regression in TensorFlow"]},{"cell_type":"code","metadata":{"id":"a7dV85x71sOe"},"source":["#Model y=X*W + b\n","#Model function\n","def output(x):\n","    return W*x + b\n","\n","#Loss function Reduce mean square\n","def loss_function(y_pred, y_true):\n","    return tf.reduce_mean(tf.square(y_pred - y_true))\n","\n","#Initialize Weights\n","W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n","b = tf.Variable(tf.ones(shape=(1,)))\n","\n","#Optimization\n","## Writing training/learing loop with GradienTape\n","learning_rate = 0.000000001\n","steps = 200 #epochs\n","\n","for i in range(steps):\n","    with tf.GradientTape() as tape:\n","        predictions = output(x_train)\n","        loss = loss_function(predictions,y_train)\n","        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n","    W.assign_sub(learning_rate * dloss_dw)\n","    b.assign_sub(learning_rate * dloss_db)\n","    print(f\"epoch : {i}, loss  {loss.numpy()},  W : {W.numpy()}, b  {b.numpy()}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3IkkgUU91wPm"},"source":["print('Final w ', W)\n","print('Final b ', b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kuiwBkJ17Qw"},"source":["# Logistic Regression in TensorFlow\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"OAT1rEaa1zRr"},"source":["# This step is for data creation\n","x_train= np.random.rand(100,1)\n","y_train=np.array([0 if i < 0.5 else 1 for i in x_train]).reshape(-1,1)\n","\n","import matplotlib.pyplot as plt\n","plt.title(\"x_train vs y_train data\")\n","plt.plot(x_train, y_train, 'b.',)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptKoTuAZ2GlL"},"source":["#Model y=sigmoid(X*W + b)\n","# same as the linear regression just sigmoid wrapped around the linear equation\n","def output(x): \n","    return tf.sigmoid(W*x + b)\n","\n","#Loss function : sum of squares\n","def loss_function(y_pred, y_true):\n","    return tf.reduce_sum(tf.square(y_pred - y_true))\n","\n","#Initialize Weights\n","W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n","b = tf.Variable(tf.zeros(shape=(1,)))\n","\n","## Optimization\n","learning_rate = 0.1\n","steps = 300 #epochs\n","\n","for i in range(steps):\n","    with tf.GradientTape() as tape:\n","        predictions = output(x_train)\n","        loss = loss_function(y_train, predictions)\n","        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n","    W.assign_sub(learning_rate * dloss_dw)\n","    b.assign_sub(learning_rate * dloss_db)\n","    print(f\"epoch : {i}, loss  {loss.numpy()},  W : {W.numpy()}, b  {b.numpy()}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lcN8ndhg2Lew"},"source":["# Keras"]},{"cell_type":"code","metadata":{"id":"ti2-R54q2JND"},"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eTJ0UGQi2RXI"},"source":["## The data, shuffled and split between train and test sets\n","(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n","num_classes=10\n","x_train = X_train.reshape(60000, 784)\n","x_test = X_test.reshape(10000, 784)\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","## Convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(Y_train, num_classes)\n","y_test = keras.utils.to_categorical(Y_test, num_classes)\n","\n","print(x_train.shape, 'train input samples')\n","print(x_test.shape, 'test input samples')\n","\n","print(y_train.shape, 'train output samples')\n","print(y_test.shape, 'test output samples')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7YnoPKF2R_G"},"source":["model = keras.Sequential()\n","#The first hidden layer. The model needs to know what input shape it should expect. \n","#For this reason, the first layer in a Sequential model needs to receive information about its input shape.\n","#Only the first need the shape information, because following layers can do automatic shape inference\n","model.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","\n","#The dense layer is simply a layer where each unit or neuron is connected to each neuron in the next layer.\n","model.add(layers.Dense(20, activation='sigmoid'))\n","\n","#In the final layer mention the output classes\n","model.add(layers.Dense(10, activation='softmax'))\n","\n","#Model Summary\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k862pHak2Wmd"},"source":["# Compiling model : we define loss function, optimizer and validation matric of our choice\n","model.compile(loss='mean_squared_error', metrics=['accuracy'])\n","\n","# Fit method: actually running our model by supplying our input and validation data\n","model.fit(x_train, y_train,epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgDgrwVa2YIx"},"source":["#print(model.get_weights())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hb5dzT6G2Ze5"},"source":["loss, acc = model.evaluate(x_test,  y_test, verbose=1)\n","print(\"Test Accuracy\", acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GtGnGOfv2a21"},"source":[""],"execution_count":null,"outputs":[]}]}