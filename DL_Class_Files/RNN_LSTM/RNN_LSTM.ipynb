{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN_LSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOVw9Zye/UyRalbdfPObDpO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"XzDOhNE5WUai"},"source":["#Importing dependencies\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import tensorflow.keras as keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import RNN, SimpleRNN, LSTM\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import random\n","import urllib.request  \n","import matplotlib.pyplot as plt\n","%matplotlib inline \n","#For plotting the matplotlib graphs in notebook"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PfjAvDHEWUap"},"source":["## LAB1: Manual Sequential Model"]},{"cell_type":"code","metadata":{"id":"DVAbdzDNWUaq"},"source":["import pandas as pd\n","column_names = ['word1', 'word2', 'word3', 'word4']\n","\n","gram2 = pd.read_csv('https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Love_Gram/love_gram.txt', delimiter='\\t', names=column_names) #Importing csv file with column names\n","gram2 = gram2.drop(['word4'], axis=1) #Dropping fourth column as we will be using only the 1st 3 columns \n","print(\"shape of data\", gram2.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8JDgn3BWUav"},"source":["print(\"Few sample records from data \\n\", gram2.sample(10))\n","print(\"\\nFrequency of word1 vlaues \\n\", gram2[\"word1\"].value_counts())\n","print(\"\\nFrequency of word2 vlaues \\n\", gram2[\"word2\"].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qei6oWj-WUay"},"source":["\"\"\"\n","Finding our words to create dictionary\n","Here we find unique values in each column and save each of those values .\n","Later which we will take the unique value for the entire appened columns\n","This will be our vocabulary list,which are the unique words in our data file\n","\"\"\"\n","chars = []\n","for i in list(gram2.columns.values):\n","    for j in pd.unique(gram2[i]):\n","        chars.append(j)\n","chars = np.unique(chars)\n","\n","\n","print('Count of unique words overall:', len(chars))\n","print('unique words list:', chars)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oODLeMyeWUa2"},"source":["\"\"\"\n","creating our word:indice pair dictionary and inverse\n","Here will be creating two dictonary values\n","char_indices : This contains each words mapped to an unique digit \n","indices_char : This contains each digits mapped to a word in the same sequence as char_indices \n","\"\"\"\n","char_indices = dict((c, i) for i, c in enumerate(chars))\n","indices_char = dict((i, c) for i, c in enumerate(chars))\n","\n","print(\"char_indices dictionary \\n\",char_indices)\n","print(\"char_indices.keys \\n\", char_indices.keys())\n","print(\"char_indices.values \\n\", char_indices.values())\n","print(\"\\n ########################################\\n\")\n","print(\"indices_char dictionary \\n\", indices_char)\n","print(\"indices_char keys \\n\",indices_char.keys())\n","print(\"indices_char values \\n\",indices_char.values())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wqmA6XwGWUa5"},"source":["\"\"\"\n","Here the column in word1 are being mapped to digits,the first 10 words is shown as an example \n","hate  => 41\n","hated => 42 and so on\n","\"\"\"\n","#### Onehot encoding of X\n","X1 = gram2['word1'].map(char_indices)\n","X1 = keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n","print(\"X1.shape is \",X1.shape)\n","\n","#Mapping y with indices\n","y1 = gram2['word2'].map(char_indices)\n","#Creatinng hot encodings for the final layer\n","#Creating one hot encoding ,meaning each digit is represented by 1's and 0's\n","#eg for 5 , 6 , 7 ,8 we represent them by [1,0,0,0] [0,1,0,0] [0,0,1,0] [0,0,0,1] these are the conventions used in frameworks\n","y1 = keras.utils.to_categorical(np.array(y1), num_classes=len(char_indices))\n","print(\"y1.shape is \",y1.shape)\n","#print(y1[0:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXXdlvgtWUa8"},"source":["#Lets take example of two words\n","print(\"The word is -->\"+gram2['word1'][0])\n","print(\"The one hot encoded version of the word is \\n\",X1[0])\n","#print(\"The one hot encoded version of the digit {0} is \\n{1}\".format(X1[0],y1[0]))\n","\n","print(\"\\nThe word is --> \"+gram2['word1'][500])\n","print(\"The one hot encoded version of the word is \\n\",X1[500])\n","#print(\"The one hot encoded version of the digit {0} is \\n{1}\".format(X1[30],y1[30]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTqrWG8dWUbA"},"source":["\"\"\"\n","We will be creating a Simple Neural Network with 10 hidden nodes in each layer,whose activation functions are sigmoid \n","and in the final layer we will be using softmax in the final layer \n","\n","Parameter count:\n","Here the parameters will be weights and bias \n","for the first layer the input_dim is one ,thus there will be 10 weights and 10 biases,therefore a total of 20 in first layer\n","for the second layer the input dim is 139,thus there will be 1390 weights and 139 biases,a total of 1529 in second layer\n","therefore a total of 1549 parameters for the overall model \n","\n","\"\"\"\n","\n","model1 = Sequential()\n","\n","model1.add(Dense(10, input_dim=X1.shape[1], activation='sigmoid'))\n","\n","model1.add(Dense(y1.shape[1] ,activation='softmax'))\n","\n","model1.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQIUL81PWUbD"},"source":["model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# Train model\n","history = model1.fit(X1, y1, epochs=20, batch_size=50,  verbose=1)\n","# Print Accuracy\n","scores = model1.evaluate(X1, y1) \n","print(\"%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w40P-F7PWUbK"},"source":["#We will see what the 1st hidden layer output representation of the data  \n","# to predict the hidden layer activations, \n","# let's rewrite first layer of our model and give it the weights from fully trained previous model\n","model1h = Sequential()\n","model1h.add(Dense(10, input_dim=X1.shape[1], weights=model1.layers[0].get_weights()))\n","model1h.add(Activation('sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lcm6vrFKWUbO"},"source":["# Getting the hidden layer activations\n","h1 = model1h.predict(X1)\n","#peak into our hidden layer activations\n","print(\"The hidden layer output for every record - Shape of it \\n\", h1.shape)\n","print(\"Few sample records from hidden layer \\n\",h1[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z4VQ7w2WWUbS"},"source":["#h1 and word2 as out input, word3 as our y variable\n","#build model [X: 10 : 139(word3)]\n","#Pridict the final results\n","X2_2 = gram2['word2'].map(char_indices)\n","X2_2 = keras.utils.to_categorical(np.array(X2_2), num_classes=len(char_indices))\n","X2_2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHOKQaGjWUbV"},"source":["\"\"\"\n","We append the input words of the words2 column in the output of the h1 layer,this gives us the combined input representation\n","\"\"\"\n","X2 = np.append(h1,X2_2, axis=1)\n","print(\"X2 Shape\", X2.shape)\n","#print(X2[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lITotkk1WUbY"},"source":["#Mapping y2 with indices\n","y2 = gram2['word3'].map(char_indices)\n","#Creatinng hot encodings for the final layer\n","y2 = keras.utils.to_categorical(np.array(y2), num_classes=len(char_indices))\n","print(\"y2.shape\", y2.shape)\n","#print(y2[:2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZxp5y7PWUbb"},"source":["model2 = Sequential()\n","\n","model2.add(Dense(10, input_dim=X2.shape[1], activation='sigmoid'))\n","\n","model2.add(Dense(y2.shape[1], activation='softmax'))\n","\n","model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bz9zV4dbWUbf"},"source":["model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# Train model\n","history = model2.fit(X2, y2, epochs=20, batch_size=50,  verbose=1)\n","# Print Accuracy\n","scores = model2.evaluate(X2, y2) \n","print(\"%s: %.2f%%\" % (model2.metrics_names[1], scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKdgEeRmWUbj"},"source":["# A predict function that takes input word1 and word2; and predict word3 \n","#1. take the input word , and represent them using digits from the char_indices dictonary values\n","#2. getting the intermediate hidden nodes for word1\n","#3. appending hidden activations with word2 as final test set\n","#4. prediction on this test set\n","def two_step_pred(words_in):\n","    #indices_in = [char_indices[i] for i in words_in] #converting or words into indices\n","    \n","    index_input=char_indices[words_in[0]]\n","    indices_in = keras.utils.to_categorical(index_input, num_classes=len(char_indices))\n","    indices_in=indices_in.reshape(1,len(char_indices))\n","    h1_test = model1h.predict(indices_in) # getting our intermediate hidden activations from model1h\n","    \n","    \n","    index_input2=char_indices[words_in[1]]\n","    indices_in2 = keras.utils.to_categorical(index_input2, num_classes=len(char_indices))\n","    indices_in2= indices_in2.reshape(1,len(char_indices))\n","    X2_test = np.append(h1_test, indices_in2, axis=1) #preparing final test data by appending hidden with word2\n","    \n","    \n","    yhat = np.argmax(model2.predict(X2_test), axis=-1) #predicting final output from model2\n","    return indices_char[yhat[0]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"goJvlbv_WUbl"},"source":["print(two_step_pred(['love', 'it']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owce0IeKWUbo"},"source":["print(two_step_pred(['love', 'to']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cNQndY9_WUbs"},"source":["print(two_step_pred(['love', 'the']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xz-9W78mWUbx"},"source":["## LAB: RNN for word prediction(Love gram data)"]},{"cell_type":"code","metadata":{"id":"wgGsJMPcWUby"},"source":["#Preparing our data for model\n","#We take two words columns from the csv file and then convert them to digits \n","X3 = gram2[['word1','word2']]\n","for i in list(X3.columns.values):\n","    X3[i] = X3[i].map(char_indices)\n","\n","X3=np.array(X3)\n","#print(X3)\n","#The same data is reshaped with similar structure but appended with 1 value to make it 3de array\n","X3=np.reshape(X3,(X3.shape[0],2,1))\n","#print(X3)\n","X3 = keras.utils.to_categorical(np.array(X3), num_classes=len(char_indices))\n","print(X3.shape)\n","#print(X3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-Ej9YLCWUb1"},"source":["y3 = gram2['word3'].map(char_indices)\n","y3 = keras.utils.to_categorical(np.array(y3), num_classes=len(char_indices))\n","y3.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVewILfYWUb3"},"source":["print(\"time stamps in this model\" , X3.shape[1])\n","print(\"one hot Variable\" , X3.shape[2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EDKryYFkWUb6"},"source":["model3 = Sequential()\n","\n","#model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, input data_dim)))\n","model3.add(SimpleRNN(30, input_shape=(X3.shape[1],X3.shape[2])))\n","\n","model3.add(Dense(len(char_indices), activation='softmax' ))\n","#model3.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jwQDPa5wJcLm"},"source":["# compile network\n","model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model3.fit(X3, y3, epochs=20,  validation_data=(X3, y3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kRtNQQAoWUb8"},"source":["#Save Weights\n","model3.save_weights(\"Weights_trained.hdf5\")\n","# load weights\n","\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Saved_models/RNN_Weights_trained_v10.hdf5\", \"RNN_Weights_trained_v10.hdf5\")\n","model3.load_weights(\"RNN_Weights_trained_v10.hdf5\")\n","\n","#weightsfile= \"D:\\\\Google Drive\\\\Training\\\\Datasets\\\\model_weights\\\\RNN_Weights_trained_v10.hdf5\"\n","#model3.load_weights(weightsfile)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BdIDb4YqWUcD"},"source":["# compile network\n","model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model3.fit(X3, y3, epochs=10, verbose=1, validation_data=(X3, y3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbORaddmWUcG"},"source":["# evaluate\n","# 1. Enocde the words to their resepctive digits value\n","# 2. Reshape according to input value trained to the network\n","# 3. Pass the value to the model,convert into integer while getting the word from indices_char dicitonary\n","\n","def rnn_word_pred(in_text):\n","    print(\"Input is - \" , in_text)\n","    encoded = [char_indices[i] for i in in_text]\n","    encoded = np.array(encoded).reshape(1,2,1)\n","    encoded =keras.utils.to_categorical(np.array(encoded), num_classes=len(char_indices))\n","    yhat = np.argmax(model3.predict(encoded), axis=-1)[0]\n","    print(\"Output is --> \" ,indices_char[yhat])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"thgoCphLWUcL"},"source":["rnn_word_pred([\"love\",\"the\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02YwNzxpWUcO"},"source":["rnn_word_pred([\"love\",\"it\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U04eZOBTWUcQ"},"source":["rnn_word_pred([\"love\",\"to\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mqtodj9WUcU"},"source":["rnn_word_pred([\"hate\",\"the\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-KdezQQ8WUcX"},"source":["## LSTM"]},{"cell_type":"code","metadata":{"id":"uSzEdHxcNfcR"},"source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/12_Chars_Data/3Gram_12Chars.csv\", \"3Gram_12Chars.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwlL9j5yWUcZ"},"source":["text_read = open('3Gram_12Chars.csv').read().lower()\n","print(\"Text\\n\", text_read[0:300])\n","#Replace comma with space\n","text= text_read.replace(',',' ').replace('\\r','')\n","#Unique characters in our dataset we then sort it\n","chars = sorted(list(set(text)))\n","#print(\"Unique Characters in the text \\n \",chars)\n","#\\n is character string for new line, we dont need that in our dictionary of chars\n","chars.remove('\\n')\n","#print(\"\\n Character after removing \\'\\\\n\\'\",chars)\n","#print(\"\\n All chars lenght\", len(chars))\n","char_indices = dict((c, i) for i, c in enumerate(chars))\n","#print(\"characters to indices dictionary\\n\", char_indices)\n","indices_char = dict((i, c) for i, c in enumerate(chars))\n","#print(\"indices to char dictionary\\n\", indices_char)\n","#print('unique chars: ', {len(chars)})\n","data = text.splitlines()\n","#print(data[:20])\n","#Adding a space at the end\n","data = [i+' ' for i in data]\n","#print(data[0:200])\n","sentences = [[char_indices[j] for j in i] for i in data ]\n","Seq_ln = 14\n","X = []\n","y = []\n","for i in sentences:\n","    for j in range(len(i)-Seq_ln):\n","        X.append(i[j:j+Seq_ln])\n","        y.append(i[j+Seq_ln])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_Eqea6LWUcd"},"source":["#The first row is the X's first row up to 14 character\n","#The second row is the X's first row starting from second character up to 14 character\n","#The third row is the X's first row starting from third character up to 14 character and so on \n","X=np.array(X)\n","X1=np.reshape(X,(X.shape[0],X.shape[1],1))\n","#print(X1.shape)\n","X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n","#print(X1.shape)\n","\n","#Target Variable\n","#y[:10]\n","#Reshapig our label for model\n","y1 = np.array(y)\n","# one hot encode outputs\n","y1 = keras.utils.to_categorical(np.array(y), num_classes=len(char_indices))\n","#y1.shape\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hIxTUMTtWUch"},"source":["#building the model\n","model_LSTM = Sequential()\n","#model1.add(LSTM('number of hidden nodes in each cell', input_shape=(timesteps, data_dim)))\n","model_LSTM.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]))) \n","model_LSTM.add(Dense(len(char_indices) , activation='softmax'))\n","model_LSTM.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBoyv2w-WUck"},"source":["# compile network\n","model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model_LSTM.fit(X_train, y_train, epochs=1, verbose=1, validation_data=(X_test, y_test))\n","model_LSTM.save_weights(\"char_LSTM_weights.hdf5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73rTupMgOmJp"},"source":["# load weights from a pre-trained model\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Saved_models/char_LSTM_model_weights_v1.hdf5\", \"char_rnn_model_weights_LSTM_v1.hdf5\")\n","model_LSTM.load_weights(\"char_rnn_model_weights_LSTM_v1.hdf5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e78KkMDpWUcu"},"source":["## Predictions"]},{"cell_type":"code","metadata":{"id":"NNkC4iaCWUcv"},"source":["#function to prepare test input\n","def prepare_input1(in_text):\n","    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n","    X1= keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n","    return(X1)\n","#function to loop our preditions\n","def complete_pred1(in_text):\n","    #original_text = in_text\n","    #generated = in_text\n","    completion = ''\n","    while True:\n","        x = prepare_input1(in_text)\n","        pred = np.argmax(model_LSTM.predict(x, verbose=0), axis=-1)[0]\n","        next_char = indices_char[pred]\n","\n","        in_text = in_text[1:] + next_char\n","        completion += next_char\n","\n","        if len(completion)> 20 or next_char == ' ':\n","            return completion"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AseN13JeWUcz"},"source":["in_text = 'whatever they '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n","in_text = 'these results '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n","in_text = 'in accordance '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n","in_text = 'knowledge and '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n","in_text = 'political and '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n","in_text = 'of particular '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzNQTwrHROWY"},"source":[""],"execution_count":null,"outputs":[]}]}