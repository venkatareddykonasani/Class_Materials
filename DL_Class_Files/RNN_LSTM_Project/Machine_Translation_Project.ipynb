{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMkXVt6GxXQtNniMhQjWGnx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2pbH_v165DfU","executionInfo":{"status":"ok","timestamp":1662978665107,"user_tz":-330,"elapsed":1638,"user":{"displayName":"Venkata Reddy Konasani (Venky)","userId":"11388081409849462561"}}},"source":["#Importing dependencies\n","import numpy as np\n","import string\n","import random\n","import re\n","import tensorflow as tf\n","import pandas as pd\n","import tensorflow.keras as keras\n","import matplotlib.pyplot as plt\n","import sklearn\n","from tensorflow.keras.models import Sequential\n","from numpy import array, argmax, random, take\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import RNN, SimpleRNN, LSTM,  Embedding, RepeatVector\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import urllib.request  \n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","%matplotlib inline\n","#For plotting the matplotlib graphs in notebook"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0-urJ0OheA97"},"source":["# English to Hindi Translation"]},{"cell_type":"markdown","metadata":{"id":"Ip8W-aV4mlrl"},"source":["## Data Importing"]},{"cell_type":"code","metadata":{"id":"NqaWH4jIBYQq"},"source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Language_Translation/eng_hin/eng_hin.zip\", \"eng_hin.zip\")\n","!unzip -qq 'eng_hin.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4emzmzyXd_-V"},"source":["raw_data= open(\"eng_hin.txt\", mode='rt', encoding='utf-8').read()\n","raw_data=raw_data.strip().split('\\n')\n","raw_data=[i.split('\\t') for i in raw_data]\n","lang1_lang2_data=array(raw_data)\n","print(lang1_lang2_data)\n","print(\" \\n Overall Sentence pairs\", len(lang1_lang2_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Si_lW_Atestu"},"source":["## Data Pre-processing"]},{"cell_type":"code","metadata":{"id":"VcoFm1XielwM"},"source":["# Remove punctuation\n","lang1_lang2_data[:,0] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,0]]\n","lang1_lang2_data[:,1] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,1]]\n","\n","print(lang1_lang2_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z51GGJMMerr0"},"source":["## convert text to lowercase\n","for word in range(len(lang1_lang2_data)):\n","    lang1_lang2_data[word,0] = lang1_lang2_data[word,0].lower()\n","    lang1_lang2_data[word,1] = lang1_lang2_data[word,1].lower()\n","print(lang1_lang2_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1ABakvHe2_0"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lang1_lang2_data[:, 0])\n","lang1_tokens=tokenizer\n","lang1_vocab_size = len(lang1_tokens.word_index) + 1\n","print(\"lang1_vocab_size\", lang1_vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6Gl6S-2e9wM"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lang1_lang2_data[:, 1])\n","lang2_tokens=tokenizer\n","lang2_vocab_size = len(lang2_tokens.word_index) + 1\n","print(\"lang2_vocab_size\", lang2_vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y5grARf9e1Zl"},"source":["## Train and Test Data"]},{"cell_type":"code","metadata":{"id":"yUb3Q9Gde90C"},"source":["# split data into train and test set\n","train, test = train_test_split(lang1_lang2_data, test_size=0.1, random_state = 44)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8w1KW8Ule97Q"},"source":["# Maximum number of words in a sentence in  Lang1 and Lang2\n","lang1_seq_length=15 \n","lang2_seq_length=15\n","\n","X_train_seq=lang1_tokens.texts_to_sequences(train[:, 0])\n","X_train= pad_sequences(X_train_seq,lang1_seq_length,padding='post')\n","\n","Y_train_seq=lang2_tokens.texts_to_sequences(train[:, 1])\n","Y_train= pad_sequences(Y_train_seq,lang2_seq_length,padding='post')\n","\n","X_test_seq=lang1_tokens.texts_to_sequences(test[:, 0])\n","X_test= pad_sequences(X_test_seq,lang1_seq_length,padding='post')\n","\n","Y_test_seq=lang2_tokens.texts_to_sequences(test[:, 1])\n","Y_test= pad_sequences(Y_test_seq,lang2_seq_length,padding='post')\n","\n","print(\"X_train.shape\", X_train.shape)\n","print(\"Y_train.shape\",Y_train.shape)\n","print(\"X_test.shape\",X_test.shape)\n","print(\"Y_test.shape\", Y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dWuUWBiSe94r"},"source":["#Sample Text\n","print(\"Text data\", [train[5, 0]])\n","print('Numbers sequence', X_train_seq[5])\n","print('Padded Sequence', X_train[5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MmGjm08Te03u"},"source":["## Model building"]},{"cell_type":"code","metadata":{"id":"U83CEs6WfaI3"},"source":["model = Sequential()\n","model.add(Embedding(input_dim=lang1_vocab_size, output_dim=256, input_length=lang1_seq_length, mask_zero=True))\n","# input_dim - Size of the vocabulary\n","# output_dim - Embedding Vector Length\n","# input_length - Length of the input sequence\n","# mask_zero=True for zero padded inputs - it means, ignore zero while training\n","\n","model.add(LSTM(128)) \n","# Encoding\n","\n","model.add(RepeatVector(lang2_seq_length)) \n","# Resultant Thought vector after encoding\n","\n","model.add(LSTM(128, return_sequences=True)) \n","#Decoding\n","#return_sequences #True - Return the output values at each time step #False -last time step output only\n","\n","\n","model.add(Dense(lang2_vocab_size, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDU-o4BnfaSS"},"source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5gXe_Z44faPp"},"source":["model.fit(X_train, Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1),  \n","                    epochs=1, verbose=1, \n","                    batch_size=1024)\n","model.save_weights('Eng_Hin_model1.hdf5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zOuzIjb_faNB"},"source":["#Download Pre-trained model\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","downloaded = drive.CreateFile({'id':\"1gplZOzfv9e_jb5c66BWWOAJCVMfuFR_I\"})   \n","downloaded.GetContentFile('Eng_hin_model.zip') \n","!unzip -qq 'Eng_hin_model.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFMHhvdbhk9e"},"source":["#Load Pre-trained model\n","model.load_weights('Eng_hin_model.hdf5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ju0a8y3fyM2"},"source":["## Prediction"]},{"cell_type":"code","metadata":{"id":"mxD7rurGf2__"},"source":["#Define prediction function this involves five steps Explained below\n","def one_line_prediction(text1):\n","    #1.Given below is the code for pre-processing.  \n","    def to_lines(text):\n","          sents = text.strip().split('\\n')\n","          sents = [i.split('\\t') for i in sents]\n","          return sents\n","    small_input = to_lines(text1)\n","    small_input = array(small_input)\n","    \n","    #1.1 Remove punctuation\n","    small_input[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in small_input[:,0]]\n","    \n","    #1.2 convert text to lowercase\n","    for i in range(len(small_input)):\n","        small_input[i,0] = small_input[i,0].lower()\n","\n","    #2. Encode and pad sequences to send it as input\n","    small_input_seq=lang1_tokens.texts_to_sequences(small_input[0])\n","    small_input= pad_sequences(small_input_seq,lang1_seq_length,padding='post')\n","\n","    #3. Actual prediction from model - Result will be numbers\n","    pred_seq = model.predict_classes(small_input[0:1].reshape((small_input[0:1].shape[0],small_input[0:1].shape[1])))\n","    \n","    #4. Functon for converting numbers into words based on word_index\n","    def num_to_word(n, tokens):\n","          for word, index in tokens.word_index.items():\n","              if index == n:\n","                  return word\n","          return None\n","\n","    #5. Final language-2 text after conversion from all numbers to words using above function\n","    Lang2_text = []\n","    for word_num in pred_seq:\n","          sing_pred = [] #Single word prediction \n","          for i in range(len(word_num)):\n","                t = num_to_word(word_num[i], lang2_tokens)\n","                if i > 0:\n","                    if (t == num_to_word(word_num[i-1], lang2_tokens)) or (t == None): \n","                      #Special cases like \"blank\" and \"end of the line\" in the input sequence \n","                        sing_pred.append('')  \n","                    else:\n","                        sing_pred.append(t) #Appending Single word prediction \n","          Lang2_text.append(' '.join(sing_pred))\n","    return(Lang2_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zw5owrWJf4BF"},"source":["#Get the predictions\n","Input_sentences=[\"have a great Good day\",\n","                 \"Do you speak English\",\n","                 \"I do not know your language\",\n","                 \"I need help\",\n","                 \"Thank you very much\",\n","                 \"Where can I get this\",\n","                 \"How much does it cost\",\n","                 \"Where is the bathroom\",\n","                 \"Where is the ATM\",\n","                 \"I am a visitor here\",\n","                 \"Excuse me\",\n","                 \"What do you do for living\",\n","                 \"Here is my passport\"]\n","\n","for sent in Input_sentences:\n","  print([sent] , \" -->\",one_line_prediction(sent))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcrKak-IGDKi"},"source":["#Get the predictions\n","Input_sentences=[\"I want a dog\"]\n","\n","for sent in Input_sentences:\n","  print([sent] , \" -->\",one_line_prediction(sent))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6Ss2rTeP3Xq"},"source":["#Get the predictions\n","Input_sentences=[\"I want to purchase a car\"]\n","\n","for sent in Input_sentences:\n","  print([sent] , \" -->\",one_line_prediction(sent))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rjhrc0CkFrx_"},"source":["# More Data"]},{"cell_type":"markdown","metadata":{"id":"Pdp6dR3WFjpC"},"source":["Tab-delimited Bilingual Sentence Pairs\n","\n","http://www.manythings.org/anki/"]}]}